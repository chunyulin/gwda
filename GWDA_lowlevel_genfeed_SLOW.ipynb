{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H5_FILE = \"white_h_8192_dm2.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###\n",
    "###  TF Dataset loader for GWDA\n",
    "###\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py \n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with h5py.File(H5_FILE, 'r') as f:\n",
    "    print( \"Number of template, /train: \", len(f['/train_hp']) )\n",
    "    print( \"Number of template, /val  : \", len(f['/val_hp']) )\n",
    "    print( \"Number of template, /test : \", len(f['/test_hp']) )\n",
    "\n",
    "\n",
    "class generator_gwda:\n",
    "    def __init__(self, file, tag, amp, shift=0, noise_realization=2, noise=1):\n",
    "        self.file = file\n",
    "        self.amp   = amp\n",
    "        self.tag   = tag\n",
    "        self.shift = shift\n",
    "        self.nreal = noise_realization\n",
    "        np.random.seed(None)\n",
    "    def __call__(self):\n",
    "        \n",
    "        with h5py.File(self.file, 'r') as f:\n",
    "            LEN=len(f['%s_hp'%self.tag][0])\n",
    "            for hp, m1, m2 in zip(f['%s_hp'%self.tag], f['%s_m1'%self.tag], f['%s_m2'%self.tag] ):\n",
    "                \n",
    "                ## shifted waveform to the reference point\n",
    "                shifted = np.zeros(LEN)\n",
    "                a = int((np.random.random()-0.5) * self.shift)\n",
    "                shifted[max(0,a):min(LEN, LEN+a)] += self.amp * np.roll(hp, a)[max(0,a):min(LEN, LEN+a)]\n",
    "\n",
    "                ## add noise realizations\n",
    "                data =  np.vstack( \n",
    "                    ( np.random.normal(0, 1, (1,LEN)) + shifted,\n",
    "                      np.random.normal(0, 1, (self.nreal,LEN))\n",
    "                    )).astype(np.float32)\n",
    "                \n",
    "                ## labels: such as mass, spin,...\n",
    "                m = np.zeros((self.nreal + 1, 2))\n",
    "                m[0,:] = [m1,m2]\n",
    "\n",
    "                yield data, m\n",
    "\n",
    "BATCH = 2               \n",
    "EXTF  = 2   # extend factor                \n",
    "\n",
    "iterator     = tf.data.Iterator.from_structure(\n",
    "    (tf.float32, tf.bool), \n",
    "    (tf.TensorShape([None, 8192]), tf.TensorShape([None]))\n",
    "  )\n",
    "\n",
    "next_element = iterator.get_next()\n",
    "    \n",
    "###\n",
    "amp=1.0\n",
    "ds_train  = tf.data.Dataset.from_generator( \n",
    "        generator_gwda(H5_FILE, tag=\"train\", amp=amp, shift=2000), \n",
    "        (tf.float32, tf.float32),\n",
    "        (tf.TensorShape([None, 8192]), tf.TensorShape([None, 2]) )    ## not needed actually  \n",
    "    )\n",
    "ds_train = ds_train.flat_map(lambda x,y: (tf.data.Dataset.zip( (\n",
    "        tf.data.Dataset.from_tensor_slices(x), tf.data.Dataset.from_tensor_slices( tf.cast( y[:,0], tf.bool ) )\n",
    "        #tf.data.Dataset.from_tensor_slices(x), tf.data.Dataset.from_tensor_slices( y[:,0] )\n",
    "    ) ) ) )\n",
    "ds_train = ds_train.shuffle(10*BATCH).repeat().batch(BATCH).prefetch(10*BATCH)\n",
    "##ds_train_iter = ds_train.make_one_shot_iterator().get_next()\n",
    "\n",
    "###\n",
    "amp=1.0\n",
    "ds_val  = tf.data.Dataset.from_generator( \n",
    "        generator_gwda(H5_FILE, tag=\"val\", amp=amp, shift=2000), \n",
    "        (tf.float32, tf.float32), (tf.TensorShape([None, 8192]), tf.TensorShape([None, 2]) )\n",
    "    )\n",
    "ds_val = ds_val.flat_map(lambda x,y: (tf.data.Dataset.zip( (\n",
    "        tf.data.Dataset.from_tensor_slices(x), tf.data.Dataset.from_tensor_slices( tf.cast( y[:,0], tf.bool ) )   ## How to reshape the 2nd argument??\n",
    "        #tf.data.Dataset.from_tensor_slices(x), tf.data.Dataset.from_tensor_slices( y[:,0] )\n",
    "    ) ) ) )\n",
    "ds_val = ds_val.shuffle(EXTF*BATCH).repeat().batch(BATCH).prefetch(EXTF*BATCH)\n",
    "\n",
    "### make a single iterator for train/val/test set with the same shape and type.\n",
    "#iterator     = tf.data.Iterator.from_structure(ds_train.output_types, ds_train.output_shapes)\n",
    "\n",
    "train_dsi_init = iterator.make_initializer(ds_train)\n",
    "val_dsi_init = iterator.make_initializer(ds_val)\n",
    "\n",
    "# Here is an example on how to read elements from the GWDA dataset\n",
    "with tf.Session() as sess:\n",
    "    sess.run(train_dsi_init)\n",
    "    d, l = sess.run(next_element)\n",
    "    print (d, l)\n",
    "    \n",
    "    sess.run(val_dsi_init)\n",
    "    d, l = sess.run(next_element)\n",
    "    print (d, l)\n",
    "    \n",
    "    sess.run(train_dsi_init)\n",
    "    d, l = sess.run(next_element)\n",
    "    print (d, l)\n",
    "    #print (label)\n",
    "    #plt.plot(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###\n",
    "### Batch in, loss/prediction out\n",
    "###\n",
    "from tensorflow.contrib.learn import ModeKeys\n",
    "tf.reset_default_graph()\n",
    "\n",
    "iterator     = tf.data.Iterator.from_structure(\n",
    "    (tf.float32, tf.bool), \n",
    "    (tf.TensorShape([None, 8192]), tf.TensorShape([None]))\n",
    "  )\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "\n",
    "LRATE = 1e-4\n",
    "\n",
    "(xbatch, ybatch) = next_element\n",
    "\n",
    "#tf.reset_default_graph()\n",
    "DIM   = 8192\n",
    "\n",
    "# ============================================== output logits\n",
    "feature = tf.reshape(xbatch, [-1, DIM,1])\n",
    "ybatch = tf.reshape(ybatch, [-1, 1])\n",
    "\n",
    "args = {\"padding\":'valid', \"activation\":None,\n",
    "        \"kernel_initializer\":tf.truncated_normal_initializer(), \n",
    "        \"bias_initializer\":tf.zeros_initializer()     }\n",
    "\n",
    "with tf.variable_scope('', reuse=tf.AUTO_REUSE):\n",
    "    def convl(in_, F, K, D, S, PO, PS, act, name):\n",
    "        out = tf.layers.conv1d( in_, filters=F, kernel_size=K, dilation_rate=D, strides=S, **args)\n",
    "        out = tf.layers.max_pooling1d(out, pool_size=PO, strides=PS, padding='valid')\n",
    "        return act(out)\n",
    "\n",
    "    o1 = convl(feature, F=16, K=16, D=1, S=1, PO=4, PS=4, act=tf.nn.relu, name=\"conv1\")\n",
    "    o2 = convl(o1,      F=32, K=8,  D=4, S=1, PO=4, PS=4, act=tf.nn.relu, name=\"conv2\")\n",
    "    o3 = convl(o2,      F=64, K=8,  D=4, S=1, PO=4, PS=4, act=tf.nn.relu, name=\"conv3\")\n",
    "\n",
    "    dim = o3.get_shape().as_list()\n",
    "    fcnn = dim[1]*dim[2]\n",
    "    o4 = tf.reshape(o3, [-1, fcnn])\n",
    "    o4     = tf.layers.dense(o4, 64, activation=tf.nn.relu, name=\"fc1\")\n",
    "    logits = tf.layers.dense(o4, 1, activation=None, name=\"logit\")\n",
    "    # ================================================ End of Network\n",
    "\n",
    "    ## with reduction compared to tf.nn.softmax_cross_entropy_with_logits_v2 \n",
    "    loss_op = tf.losses.sigmoid_cross_entropy(logits=logits, multi_class_labels=ybatch)\n",
    "\n",
    "    #optimizer = tf.train.AdadeltaOptimizer(LRATE, rho=0.90, epsilon=1e-08).minimize(loss_op)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(LRATE).minimize(loss_op)\n",
    "    optimizer = tf.train.AdamOptimizer(LRATE).minimize(loss_op)\n",
    "\n",
    "    # Compute predictions\n",
    "    predict_prob = tf.sigmoid(logits)\n",
    "    predict      = tf.cast( tf.round(predict_prob), tf.int32 )\n",
    "    \n",
    "\n",
    "    accuracy_op, accuracy    = tf.metrics.accuracy(labels=ybatch, predictions=predict)\n",
    "    _, sensitivity = tf.metrics.recall(labels=ybatch, predictions=predict)\n",
    "    #    #_, sensitivity = tf.metrics.sensitivity_at_specificity(labels=y, predictions=predict, specificity=0.005)\n",
    "    #\n",
    "    #_, fp = tf.metrics.false_positives(labels=ybatch, predictions=predict)\n",
    "    #_, fn = tf.metrics.false_negatives(labels=ybatch, predictions=predict)\n",
    "    #_, tp = tf.metrics.true_positives(labels=ybatch, predictions=predict)\n",
    "    #_, tn = tf.metrics.true_negatives(labels=ybatch, predictions=predict)\n",
    "\n",
    "  \n",
    "for var in tf.local_variables():\n",
    "    print (var)\n",
    "    \n",
    "tf.summary.scalar('loss', loss_op)\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "#tf.summary.scalar('recall/false_negatives',  tf.get_default_graph().get_tensor_by_name(\"recall/false_negatives/count:0\") )\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_NOISE=30\n",
    "\n",
    "DATA_LEN = 700*(NUM_NOISE+1)\n",
    "BATCH = 128\n",
    "ROOT_FOLDER = '/tmp/tf_tmp'\n",
    "\n",
    "def train_for_amp(amp, BATCH=128, EPOCHS=100, MONITOR=2, PATIENCE=4, TOLLERENCE=1.e-7):\n",
    "\n",
    "    ###  Data-set\n",
    "    MAXSHIFT= 0\n",
    "    \n",
    "    print(\"Trainning for A= %f\"% (amp ))\n",
    " \n",
    "    ##################  train dataset ##################  TODO: to move into a function\n",
    "    dst = tf.data.Dataset.from_generator( \n",
    "            generator_gwda(H5_FILE, 'train', amp, MAXSHIFT, noise_realization=NUM_NOISE), \n",
    "            (tf.float32, tf.float32),\n",
    "            (tf.TensorShape([None, 8192]), tf.TensorShape([None, 2]) )    ## not needed actually  \n",
    "        )\n",
    "    dst = dst.flat_map(lambda x,y: (tf.data.Dataset.zip( (\n",
    "            tf.data.Dataset.from_tensor_slices(x), tf.data.Dataset.from_tensor_slices( tf.cast(y[:,0], tf.bool ) )\n",
    "        ) ) ) )\n",
    "    dst  = dst.shuffle(BATCH).repeat().batch(BATCH).prefetch(BATCH)\n",
    "    ##################  validation dataset ##################\n",
    "    dsv = tf.data.Dataset.from_generator(\n",
    "        generator_gwda(H5_FILE, 'val', amp, MAXSHIFT, noise_realization=1), \n",
    "            (tf.float32, tf.float32),\n",
    "            (tf.TensorShape([None, 8192]), tf.TensorShape([None, 2]) )    ## not needed actually  \n",
    "        )\n",
    "    dsv = dsv.flat_map(lambda x,y: (tf.data.Dataset.zip( (\n",
    "            tf.data.Dataset.from_tensor_slices(x), tf.data.Dataset.from_tensor_slices( tf.cast(y[:,0], tf.bool ))\n",
    "            ) ) ) )\n",
    "    dsv = dsv.shuffle(BATCH).batch(BATCH).prefetch(BATCH)\n",
    "    ##################            ##################\n",
    "\n",
    "    ### make a single iterator for train/val/test set with the same shape and type.\n",
    "    #iterator     = tf.data.Iterator.from_structure(ds_train.output_types, ds_train.output_shapes)\n",
    "    \n",
    "    train_dsi_init = iterator.make_initializer(dst)\n",
    "    val_dsi_init = iterator.make_initializer(dsv)\n",
    "\n",
    "\n",
    "    ##### summarize to a new folder\n",
    "    ##train_writer = tf.summary.FileWriter(\"%s/train_%4.2f\" % (ROOT_FOLDER, amp ) )\n",
    "    ##train_writer.add_graph(tf.get_default_graph())\n",
    "    ##print('Saving graph to: %s' % ROOT_FOLDER)\n",
    "\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run( [tf.global_variables_initializer(), tf.local_variables_initializer() ] )\n",
    "\n",
    "        merged = tf.summary.merge_all()   ## operator for collecting TF summary\n",
    "        \n",
    "        patience = 0\n",
    "        time0 = time.time()\n",
    "        for e in range(EPOCHS):\n",
    "        \n",
    "            sess.run(train_dsi_init)\n",
    "            STEPS   = int(DATA_LEN / BATCH)\n",
    "            for i in range(STEPS):\n",
    "                _, summary = sess.run( [optimizer, merged] )\n",
    "                train_writer.add_summary(summary, global_step=e)\n",
    "                #print(\"Step: %d, %d\" % (i, l) )\n",
    "\n",
    "            sess.run(val_dsi_init)\n",
    "            ### evaluate    \n",
    "            loss, acc, sen = sess.run( [loss_op, accuracy, sensitivity] )\n",
    "\n",
    "            if e % MONITOR == 0:\n",
    "                duration = time.time() - time0\n",
    "                speed = STEPS * BATCH * (e+1) / duration\n",
    "                print('  Epoch: %3d, loss: %10.3e acc: %4.2g sen: %4.2f sec: %8.1f speed: %7.1f wf/sec' \n",
    "                      % (e, loss, acc, sen, duration, speed) )\n",
    "            if loss < TOLLERENCE:\n",
    "                if patience > PATIENCE: break\n",
    "                patience += 1\n",
    "            else:\n",
    "                patience = 0\n",
    "\n",
    "        # Save model (variables)\n",
    "        save_path = saver.save(sess, \"%s/model_%4.2f.ckpt\" % (ROOT_FOLDER, amp ) )\n",
    "        print(\"Model saved at %s\" % save_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###\n",
    "### Main program\n",
    "###\n",
    "### if __name__ == '__main__':\n",
    "\n",
    "np.random.seed(1)\n",
    "    \n",
    "##\n",
    "##  Training with fixed template ....\n",
    "##\n",
    "###\n",
    "saver = tf.train.Saver(max_to_keep=50)\n",
    "\n",
    "TEST_LIST = [1.8, 1.7, 1.6, 1.5, 1.4, 1.3, 1.2, 1.1, 1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.0]\n",
    "TRAIN_A   = [1.1, 1.0, 0.9, 0.8, 0.75, 0.7, 0.65, 0.6, 0.55, 0.54, 0.53, 0.52, 0.51, 0.5]\n",
    "#TEST_LIST = [1.0]\n",
    "#TRAIN_A   = [1.0]\n",
    "\n",
    "\n",
    "## preload model here ?\n",
    "\n",
    "for amp in TRAIN_A:\n",
    "    ## summarize to a new folder for each Amp\n",
    "    train_writer = tf.summary.FileWriter(\"%s/train_%4.2f\" % (ROOT_FOLDER, amp ) )\n",
    "    train_writer.add_graph(tf.get_default_graph())\n",
    "    print('Saving graph to: %s' % ROOT_FOLDER)\n",
    "\n",
    "    train_for_amp(amp)\n",
    "\n",
    "    save_path = saver.save(sess, \"%s/model_%4.2f.ckpt\" % (ROOT_FOLDER, amp ) )\n",
    "    print(\"Model saved at %s\" % save_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
